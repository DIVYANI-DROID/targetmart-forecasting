{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8eacbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load synthetic data\n",
    "df = pd.read_csv(\"../data/sample_sales.csv\", parse_dates=[\"date\"])\n",
    "print(df.head())\n",
    "\n",
    "# Plot sales over time\n",
    "plt.figure(figsize=(10,5))\n",
    "for product in df['product'].unique():\n",
    "    plt.plot(df[df['product'] == product]['date'], df[df['product'] == product]['units_sold'], label=f'Product: {product}')\n",
    "plt.title(\"Sales Trend per Product\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51757d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"/Users/da/Documents/TARGET/targetmart-forecasting/data/sample_sales.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "# Lag feature: previous day's sales\n",
    "df[\"sales_lag_1\"] = df.groupby(\"product\")[\"units_sold\"].shift(1)\n",
    "\n",
    "# Sort values (important for time series)\n",
    "df.sort_values(by=[\"product\", \"date\"], inplace=True)\n",
    "\n",
    "# Rolling average features\n",
    "df[\"rolling_avg_3\"] = df.groupby(\"product\")[\"units_sold\"].shift(1).rolling(window=3).mean()\n",
    "df[\"rolling_avg_7\"] = df.groupby(\"product\")[\"units_sold\"].shift(1).rolling(window=7).mean()\n",
    "\n",
    "# Promo flag as categorical (optional but helps with models like XGBoost)\n",
    "df[\"promo_flag\"] = df[\"promo\"].astype(\"category\")\n",
    "\n",
    "# Day of the week\n",
    "df[\"day_of_week\"] = df[\"date\"].dt.dayofweek # 0 = Monday, 6 = Sunday\n",
    "\n",
    "# Weekend indicator\n",
    "df[\"is_weekend\"] = df[\"is_weekend\"] = df[\"day_of_week\"].isin([5,6]).astype(int)\n",
    "\n",
    "# Price elasticity proxy: % change in sales per unit change in price\n",
    "df[\"price_change\"] = df.groupby(\"product\")[\"price\"].pct_change()\n",
    "df[\"sales_change\"] = df.groupby(\"product\")[\"units_sold\"].pct_change()\n",
    "df[\"price_elasticity\"] = df[\"sales_change\"] / df[\"price_change\"]\n",
    "\n",
    "# Drop NA rows generated by lag/rolling\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Replace infinities from price_elasticity calc\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.dropna(inplace=True)  # or use imputation if needed\n",
    "\n",
    "# Preview\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ad395b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Date range in df:\", df[\"date\"].min(), \"to\", df[\"date\"].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e30102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from datetime import timedelta\n",
    "\n",
    "# Sort data\n",
    "df = df.sort_values([\"product\", \"date\"])\n",
    "\n",
    "# Set forecast horizon (last N days as test)\n",
    "forecast_days = 7\n",
    "\n",
    "# Split train-test set by date for each product\n",
    "train_df = pd.DataFrame()\n",
    "test_df = pd.DataFrame()\n",
    "\n",
    "for product in df[\"product\"].unique():\n",
    "    product_data = df[df[\"product\"] == product].copy()\n",
    "    cutoff_date = product_data[\"date\"].max() - timedelta(days=forecast_days)\n",
    "\n",
    "    train_df = pd.concat([train_df, product_data[product_data[\"date\"] <= cutoff_date]])\n",
    "    test_df = pd.concat([test_df, product_data[product_data[\"date\"] > cutoff_date]])\n",
    "\n",
    "# Baseline model: predict sales = previous day's sales (lag-1)\n",
    "test_df[\"baseline_pred\"] = test_df[\"sales_lag_1\"]\n",
    "\n",
    "# Evaluate using MAPE\n",
    "mape = mean_absolute_percentage_error(test_df[\"units_sold\"], test_df[\"baseline_pred\"])\n",
    "print(f\"Baseline Forecast MAPE: {mape:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab07d439",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Date range in df:\", df[\"date\"].min(), \"to\", df[\"date\"].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b9a50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# split_date = \"2023-01-09\"\n",
    "split_date = df[\"date\"].iloc[int(len(df) * 0.8)] # 80% train, 20% test\n",
    "\n",
    "train_df = df[df[\"date\"] < split_date].copy()\n",
    "test_df = df[df[\"date\"] >= split_date].copy()\n",
    "\n",
    "# Lag-based baseline\n",
    "test_df[\"baseline_pred\"] = test_df[\"sales_lag_1\"]\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "\n",
    "\n",
    "# Encode prodcut labels (if needed)\n",
    "\n",
    "#le = LabelEncoder()\n",
    "#train_df[\"product_enc\"] = le.fit_transform(train_df[\"product\"])\n",
    "#test_df[\"product_enc\"] = le.transform(test_df[\"product\"])\n",
    "\n",
    "# Error: ValueError: y contains previously unseen labels: 'Shampoo'\n",
    "#because the LabelEncoder was fitted on the training set only, \n",
    "# but our test set (test_df) contains new product(s) like 'Shampoo' that weren’t seen during \n",
    "# training — and LabelEncoder can’t handle that by default.\n",
    "\n",
    "# Fix: Use fit_transform on full data and then split\n",
    "# This way, all product categories (train + test) are included in the encoding. \n",
    "\n",
    "# Combine for consistent encoding\n",
    "# all_products = pd.concat([train_df[\"product\"], test_df[\"product\"]], axis=0)\n",
    "all_products = pd.concat([train_df[\"product\"], test_df[\"product\"]])\n",
    "\n",
    "# Encode product using full data to avoid unseen label error\n",
    "le = LabelEncoder().fit(all_products)\n",
    "\n",
    "# Now transforming separately\n",
    "\n",
    "train_df[\"product_enc\"] = le.transform(train_df[\"product\"])\n",
    "test_df[\"product_enc\"] = le.transform(test_df[\"product\"])\n",
    "\n",
    "# Define features and target\n",
    "feature_cols = [\n",
    "    \"price\", \"promo\", \"promo_flag\", \"day_of_week\", \"is_weekend\",\n",
    "    \"sales_lag_1\", \"rolling_avg_3\", \"rolling_avg_7\",\n",
    "    \"price_change\", \"sales_change\", \"price_elasticity\", \"product_enc\"\n",
    "]\n",
    "\n",
    "target = \"units_sold\"\n",
    "\n",
    "# Drop NAs\n",
    "train_df = train_df.dropna(subset=feature_cols + [target])\n",
    "test_df = test_df.dropna(subset=feature_cols + [target])\n",
    "\n",
    "# 🚨 Safety check\n",
    "if train_df.empty:\n",
    "    raise ValueError(\"train_df is empty after dropping NAs. \" \\\n",
    "    \"Please check your data and feature engineering steps\")\n",
    "\n",
    "# 🚨 Train separate models per product\n",
    "rf_models = {}\n",
    "for product in train_df[\"product\"].unique():\n",
    "    train_p = train_df[train_df[\"product\"] == product]\n",
    "    test_p = test_df[test_df[\"product\"] == product] \n",
    "\n",
    "    if train_p.empty or test_p.empty:\n",
    "        print(f\"Skipping {product} due to empty train or test set.\")\n",
    "        continue\n",
    "\n",
    "    # Train model\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(train_p[feature_cols], train_p[target])\n",
    "    rf_models[product] = rf\n",
    "    # Error: ValueError: Found array with 0 sample(s) (shape=(0, 12)) \n",
    "    # while a minimum of 1 is required by RandomForestRegressor.\n",
    "\n",
    "    # ...means the train_df is empty — it has zero rows, so RandomForestRegressor \n",
    "    # has no data to train on.\n",
    "\n",
    "    # Fix: Check shape of train_df\n",
    "    # print(\"train_df shape:\", train_df.shape)\n",
    "    # print(train_df.head())\n",
    "\n",
    "    # rf.fit(train_df[features], train_df[target])\n",
    "\n",
    "    # Predict\n",
    "    test_df.loc[test_p.index, \"rf_pred\"] = rf.predict(test_p[feature_cols])\n",
    "\n",
    "# Evaluate\n",
    "mape_rf = mean_absolute_percentage_error(test_df[target], test_df[\"rf_pred\"])\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "print(f\"Random Forest MAPE: {mape_rf:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720f5fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python code to visualize actual vs predicted sales\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Make sure date is datetime\n",
    "test_df[\"date\"] = pd.to_datetime(test_df[\"date\"])\n",
    "\n",
    "# Plot per product\n",
    "for product in test_df[\"product\"].unique():\n",
    "    product_test = test_df[test_df[\"product\"] == product].copy()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(product_test[\"date\"], product_test[\"units_sold\"], label=\"Actual Units Sold\", marker=\"o\")\n",
    "\n",
    "    if \"baseline_pred\" in product_test.columns:\n",
    "        plt.plot(product_test[\"date\"], product_test[\"baseline_pred\"], label=\"Predicted (Lag-1)\", marker=\"x\")\n",
    "\n",
    "    if \"rf_pred\" in product_test.columns:\n",
    "        plt.plot(product_test[\"date\"], product_test[\"rf_pred\"], label=\"Random Forest\", marker=\"^\")\n",
    "\n",
    "    plt.title(f\"Actual vs Predicted Sales for {product}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Units Sold\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 💡 Print MAPE after the plot\n",
    "    if \"baseline_pred\" in product_test.columns:\n",
    "        mape_baseline = mean_absolute_percentage_error(product_test[\"units_sold\"], product_test[\"baseline_pred\"])\n",
    "        print(f\"{product} - Baseline (Lag-1) MAPE: {mape_baseline:.2%}\")\n",
    "    if \"rf_pred\" in product_test.columns:\n",
    "        mape_rf = mean_absolute_percentage_error(product_test[\"units_sold\"], product_test[\"rf_pred\"])\n",
    "        print(f\"{product} - Random Forest MAPE: {mape_rf:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d854815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Visualization\n",
    "\n",
    "importances_df = pd.DataFrame()\n",
    "\n",
    "for product, model in rf_models.items():\n",
    "    importances = model.feature_importances_\n",
    "    product_importance = pd.DataFrame({\n",
    "        \"feature\": feature_cols,\n",
    "        \"importance\": importances,\n",
    "        \"product\": product\n",
    "    })\n",
    "    importances_df = pd.concat([importances_df, product_importance], axis=0)\n",
    "\n",
    "# Plotting feature importance per product\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=importances_df, x=\"importance\", y=\"feature\", hue=\"product\")\n",
    "plt.title(\"Feature Importance by Product\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.legend(title=\"Product\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"feature_importance_by_product.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc71ee0c",
   "metadata": {},
   "source": [
    "Based on the feature importance plot we just generated (from the Random Forest models per product), we can interpret which features are driving predictions for each product as follows:\n",
    "\n",
    "🔑 Top Features Driving Predictions\n",
    "🧴 Shampoo:\n",
    "sales_lag_1 – 📊 Most influential: sales from the previous day are strong predictors.\n",
    "\n",
    "price_elasticity – 🧮 Indicates customers are sensitive to price changes.\n",
    "\n",
    "sales_change – 📈 Recent trend in sales (change from lag to current).\n",
    "\n",
    "day_of_week, is_weekend – 🗓️ Suggests buying pattern shifts on certain days (e.g., weekends).\n",
    "\n",
    "promo – 📣 Promotions matter but less than above.\n",
    "\n",
    "🧼 Soap:\n",
    "rolling_avg_7 – 🧠 Customers seem to have steady buying habits over time.\n",
    "\n",
    "sales_lag_1 – Still a good predictor but not the most important.\n",
    "\n",
    "is_weekend – 📅 Consumers possibly buy more on weekends.\n",
    "\n",
    "price_elasticity – Moderate effect — possibly less price sensitive.\n",
    "\n",
    "promo, sales_change – Minor contributors.\n",
    "\n",
    "🪥 Toothpaste:\n",
    "sales_lag_1 – 🥇 Most important, again confirming daily consistency.\n",
    "\n",
    "rolling_avg_7 – Shows habitual buying over a week.\n",
    "\n",
    "sales_change – Captures small demand spikes/dips.\n",
    "\n",
    "price_elasticity – Also important, indicating price sensitivity.\n",
    "\n",
    "promo, day_of_week – Lesser but non-trivial.\n",
    "\n",
    "🧠 What Does This Mean?\n",
    "Lag features (especially sales_lag_1) dominate — users tend to repeat purchase patterns.\n",
    "\n",
    "Price sensitivity (price_elasticity) is consistently influential → You can optimize pricing.\n",
    "\n",
    "Temporal patterns (day of week, weekend flags) are useful for scheduling promotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58ff1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "\n",
    "def augment_data(df_product, n=30):\n",
    "    df_aug = pd.concat([df_product] * (n // len(df_product)), ignore_index=True)\n",
    "    df_aug = df_aug.sample(n=n, replace=True).reset_index(drop=True)\n",
    "\n",
    "    # Add small noise to numerical columns except 'units_sold'\n",
    "    numeric_cols = [col for col in df_aug.columns if df_aug[col].dtype in ['int64', 'float64'] and col not in ['units_sold']]\n",
    "    for col in numeric_cols:\n",
    "        noise = np.random.normal(0, 0.05, size=len(df_aug)) # 5% Gaussian noise\n",
    "        df_aug[col] = df_aug[col] * (1 + noise)\n",
    "\n",
    "    # Add noise to 'units_sold' too\n",
    "    units_noise = np.random.randint(-2, 3, size=len(df_aug)) # -2 to +2\n",
    "    df_aug['units_sold'] = np.clip(df_aug['units_sold'] + units_noise, 0, None)\n",
    "\n",
    "    return df_aug\n",
    "\n",
    "# Apply augmentation per product\n",
    "df_augmented = df.groupby('product', group_keys=False).apply(lambda x: augment_data(x, n=30)).reset_index(drop=True)\n",
    "\n",
    "print(df_augmented['product'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82233f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Encode categorical variables again for df_augmented\n",
    "le = LabelEncoder()\n",
    "df_augmented['product_enc'] = le.fit_transform(df_augmented['product'])\n",
    "\n",
    "# If you have other encodings (like 'day_of_week' or 'month'), do those too:\n",
    "df_augmented['day_of_week'] = pd.to_datetime(df_augmented['date']).dt.dayofweek\n",
    "df_augmented['month'] = pd.to_datetime(df_augmented['date']).dt.month\n",
    "\n",
    "missing = [col for col in feature_cols if col not in df_augmented.columns]\n",
    "print(\"❌ Missing columns:\", missing)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "for product in df_augmented['product'].unique():\n",
    "    print(f\"\\n🔍 Tuning model for: {product}\")\n",
    "\n",
    "    df_product = df_augmented[df_augmented['product'] == product]\n",
    "\n",
    "    X = df_product[feature_cols]\n",
    "    y = df_product['units_sold']\n",
    "\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    grid_search = GridSearchCV(rf, param_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    print(\"✅ Best Params:\", grid_search.best_params_)\n",
    "    print(\"📉 Best MAE:\", -grid_search.best_score_)\n",
    "\n",
    "    best_models[product] = grid_search.best_estimator_\n",
    "\n",
    "# Compute RMSE\n",
    "rmse = np.sqrt(mean_squared_error(test_df[\"units_sold\"], test_df[\"rf_pred\"]))\n",
    "print(f\"Random Forest RMSE: {rmse:.2f}\")\n",
    "\n",
    "X_sim = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763bdf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "model_dir = \"/Users/da/Documents/TARGET/targetmart-forecasting/models\"\n",
    "\n",
    "# ✅ Only create the correct directory\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "for product, model in best_models.items():\n",
    "    joblib.dump(model, f\"{model_dir}/best_rf_model_{product}.pkl\")\n",
    "\n",
    "joblib.dump(feature_cols, f\"{model_dir}/feature_cols.pkl\")\n",
    "\n",
    "joblib.dump(le, f\"{model_dir}/label_encoder_product.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
